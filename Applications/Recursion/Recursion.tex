\lab{Applications}{Recursive Functions}{Recursive Functions}

\objective{We will briefly discuss an important technique in programming known as recursion.}

A recursive function is one that calls itself.  A nice example of a recursive function is the Euclidean Algorithm for calculating the greatest common divisor of two integers, $a$ and $b$.

\begin{lstlisting}[style=python]
def gcd(a, b):
    assert a >= b and b >= 0
    if b == 0:
        return a
    else:
        return gcd(b, a%b) 
\end{lstlisting}

All recursive functions have to meet several criteria.  If any one of the criteria are violated, the recursion either doesn't happen at all or the recursion will never terminate (an infinite loop).
\begin{itemize}
 \item Base case.  This defines the stopping criteria for the recursive method.  In the method above, the base case is \li{if b==0}.
 \item Recursive call.  The function should call itself at sometime with new inputs.  The recursive call is \li{return gcd(b, a\%b)}.
 \item Convergence. The recursion must advance toward the base case.  This ensures we don't infinitely recurse.  Study the method above to see how it converges to the base case (try putting print statements in the method if help is needed).
\end{itemize}

Here's a simple combinatoric example. Suppose our goal is to enumerate every possible way to select $n=6$ numbers out of a list of ten numbers (note that we are talking combinations not permutations). We could use some sort of complicated \li{for} loop structure.  It would require six loops to accomplish the task.

\begin{lstlisting}
for i in ...:
    for j in ...:
        for k in ...:
            for l in ...:
            .
            .
            .
\end{lstlisting}

However, we wish to stress emphatically, that this is rarely the right way to do any problem.  What would happen if $n$ were large? Once you write more than three nested \li{for} loops you are probably skirting on the edges of intractability.  In fact, any time you attack a problem that suggests this approach you are likely attacking a problem that is inherently intractable. Sometimes, a problem of this type can be more elegantly solved using recursion.

For example in our combinatorics problem above we could write something like the following.  The solution uses recursion to make the solution more simple.

\lstinputlisting[style=fromfile]{./Source/combinations.py}

\begin{problem}
Note that just because a recursive algorithm exists does not mean that it is the correct way to solve the problem. Computing $n!$, for example, is very inefficient.   The factorial example makes this readily evident. Program a recursive function that will calculate $n!$.  For each step of the recursion, have it print what quantities are being calculated. Time it for increasing values of $n$ (but not too large).  Why is computing $n!$ recursively bad?Generally speaking recursion slows things down, so it should be avoided when possible.
\end{problem}

\begin{problem}
Write a non-recursive version of the \li{gcd} function at the beginning of this lab.  Time the performance of the recursive and non-recursive versions for larger and larger values of $a \geq b$. Generally, recursion does slow things down.  However, sometimes recursion is the best method for solving a problem.
\end{problem}


\begin{problem}
Use recursion to calculate the determinant of an array using cofactor expansions (this is known as Laplace's formula). The correct solution should only be several lines of code. You can see how simple recursive programs can be. Now time your function and compare its performance to finding the determinant using the LU decomposition that you wrote earlier. How does it fare? You will probably notice that the recursive method is much slower for even moderate values of $n$. Why is this? Laplace's formula is $O(n!)$, so even though it's very easy to code it's terribly inefficient in general.
\end{problem}