\lab{Algorithms}{Broyden's Method}{Broyden's Method}

\objective{Implement Broyden's Method and understand the tradeoff with Newton's Method}

We have already discussed using Newton's method to find the roots of an equation. Newton's method is generally very useful because of its fast convergence properties. However, Newton's method requires computation requires the explicit calculation of the derivative (or respectively jacobian matrix) at each step, which is computationally costly. There is a class of methods known as quasi-Newton methods that modify newton's method so that the derivative (jacobian) does not have to be computed at each step, thus making computations faster. This generally comes at a slower convergence speed, but the increased computation speed can make these methods more effective in many cases.

One quasi-Newton method is known as Broyden's method. We will first discuss the one-dimensional case of Broyden's method: the secant method.

The secant method begins with two initial points that are presumably close to the desired root. We can calculate the equation of the secant line between the two points $x_i$ and $x_{i-1}$ by the following:
\[
y = \frac{f(x_i)-f(x_{i-1})}{x_i-x_{i-1}}(x-x_i) + f(x_i)
\]

Now suppose that we let 

\begin{itemize}
\item Explain secant method in 1-d.
\item Explain under which conditions secant method doesn't converge
\end{itemize}

\begin{problem}
Implement secant method.  Compare empirically to Newton.(?)
\end{problem}
\begin{problem}
Implement false position problem and show why it is guaranteed to converge.  Maybe insert an historical comment here about how Chinese and Indian mathematicians were using this method more than 2000 years ago!
\end{problem}

\begin{itemize}
\item Jacobian as a generalization of the derivative
\item Note that calculating the inverse of the Jacobian is a computationally costly operation.
\end{itemize}

\begin{problem}
Implement Broyden's method that updates the Jacobian at each iteration.
\end{problem}

\begin{itemize}
\item We can mitigate the computational cost of calculating the Jacobian by calculating once and then making a ``good'' estimate for the next iteration that is much less expensive to compute.
\item Minimize the distance between $J_n$ and $J_{n-1}$ in the Frobenius norm.
\end{itemize}

\begin{problem}
Implement faster Broyden.  Compare to previous version and Newton.
\end{problem}
