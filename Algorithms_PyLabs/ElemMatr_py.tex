\lab{Algorithms}{Elementary Matrices}{Elementary Matrices}

\objective{In this section we will use elementary matrices to find the RREF and to find the LU decomposition.}

There are 3 types of elementary matrices, one for each of the
elementary operations. These matrices are easy to construct. Suppose
$A$ is an $m \times n$ matrix and you want to perform one of the
three elementary operations on $A$. You can do this be constructing
the $m \times m$ identity matrix, $I$, performing the elementary row
operation on $I$ to obtain $E$ and then multiplying $EA$. For
example, consider the matrix
\[
A = \begin{pmatrix}
a_{11}&a_{12}&a_{13}&a_{14}\\
a_{21}&a_{22}&a_{23}&a_{24}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}
\]
If we want to swap the first two rows, we can left multiply the
matrix $A$ by the Type I matrix.
\[
E = \begin{pmatrix}
0&1&0\\
1&0&0\\
0&0&1
\end{pmatrix},
\]
then
\[
E A =
\begin{pmatrix}
a_{21}&a_{22}&a_{23}&a_{24}\\
a_{11}&a_{12}&a_{13}&a_{14}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}.
\]

Now let's examine the next row operation.  If we want to multiply,
say, the second row of $A$ by the constant $b$, we can left multiply
the matrix $A$ be the Type II matrix
\[
\tilde{E} = \begin{pmatrix}
1&0&0\\
0&b&0\\
0&0&1
\end{pmatrix}.
\]
Then
\[
\tilde{E} A =
\begin{pmatrix}
a_{11}&a_{12}&a_{13}&a_{14}\\
b a_{21}&b a_{22}&b a_{23}&b a_{24}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}.
\]

Now let's examine the last row operation.  If we want to multiply,
say, the first row of $A$ by a constant $c$ and add it to the second
row, we can left multiply the matrix $A$ be the Type III matrix
\[
\widehat{E} = \begin{pmatrix}
1&0&0\\
c&1&0\\
0&0&1
\end{pmatrix}.
\]
Then
\[
\widehat{E} A =
\begin{pmatrix}
a_{11}&a_{12}&a_{13}&a_{14}\\
c a_{11} + a_{21}&c a_{12} + a_{22}&c a_{13} + a_{23}&c a_{14} + a_{24}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}.
\]

Below, the elementary matrices corresponding to each row operation is implemented in Python.
\lstinputlisting[style=fromfile, name=row_opers.py]{./Source/row_opers.py}

\section*{Programming Row Reduction Differently}

In this section, we use elementary matrices to reduce a matrix into
``row echelon form'' (REF), opposed to ``reduced row echelon form''
(RREF).  To solve a linear system, it is actually
faster to use REF and then finish with back-substitution, than it is
to compute RREF directly. By iteratively left multiplying by elementary matrices, we can reduce the matrix to REF. Consider the matrix below.
\[
\begin{pmatrix}
4&5&6&3 \\
2&4&6&4 \\
7&8&0&5
\end{pmatrix}
\]
Remember that our functions returns the elementary array corresponding to the desired row operation.  Also note that setting the type of our initial array is crucial.
\begin{lstlisting}
: import scipy as sp
: import row_opers as op
: A = sp.array([[4, 5, 6, 3],[2, 4, 6, 4],[7, 8, 0, 5]], dtype='float32')
array([[ 4.,  5.,  6.,  3.],
       [ 2.,  4.,  6.,  4.],
       [ 7.,  8.,  0.,  5.]], dtype=float32)
: A1 = sp.dot(op.cmultadd(3,1,0,-A[1,0]/A[0,0]), A); A1
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 7. ,  8. ,  0. ,  5. ]])
: A2 = sp.dot(op.cmultadd(3,2,0,-A1[2,0]/A1[0,0]), A1); A2
array([[  4.  ,   5.  ,   6.  ,   3.  ],
       [  0.  ,   1.5 ,   3.  ,   2.5 ],
       [  0.  ,  -0.75, -10.5 ,  -0.25]])
: A3 = sp.dot(op.cmultadd(3,2,1,-A2[2,1]/A2[1,1]), A2); A3
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 0. ,  0. , -9. ,  1. ]])
\end{lstlisting}

To complete RREF we would need to divide each row by its leading
coefficient.  We can do that using Type II matrices.  We leave it to
you to carry this out.
\begin{problem}
Write a function which takes as input an $n\times (n+1)$ (in other words, an \emph{augmented} matrix) array and performs the above naive row reduction to REF using elementary arrays.
\end{problem}

\section*{LU Decomposition}

Again, consider the array $A$. By iteratively left multiplying by
elementary arrays, we reduce as follows:
\begin{lstlisting}
: E1 = op.cmultadd(3,1,0,-A[1,0]/A[0,0]); E1
array([[ 1. ,  0. ,  0. ],
       [-0.5,  1. ,  0. ],
       [ 0. ,  0. ,  1. ]])
: B1 = sp.dot(E1, A)
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 7. ,  8. ,  0. ,  5. ]])
: E2 = op.cmultadd(3,2,0,-B1[2,0]/B1[0,0])
: B2 = sp.dot(E2, B1)
: E3 = op.cmultadd(3,2,1,-B2[2,1]/B2[1,1])
: U = sp.dot(E3, B2); U
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 0. ,  0. , -9. ,  1. ]])
\end{lstlisting}

Note that we have reduced the above matrix into upper-triangular
form, denoted as $U$.  Hence, we have
\[
U = E_3 E_2 E_1 A.
\]
Since the elementary matrices are invertible, we also have
\[
(E_3 E_2 E_1)^{-1} U =  A.
\]
This can be re-written as
\[
E_1^{-1} E_2^{-1} E_3^{-1} U =  A.
\]
Then we define $L$ to be
\[
L = E_1^{-1} E_2^{-1} E_3^{-1},
\]
which yields $L U = A$.
\begin{lstlisting}
: from scipy import linalg as la
: I = lambda x: la.inv(x)
: L = sp.dot(sp.dot(I(E1), I(E2)), I(E3)); L
array([[ 1.  ,  0.  ,  0.  ],
       [ 0.5 ,  1.  ,  0.  ],
       [ 1.75, -0.5 ,  1.  ]])
: sp.dot(L, U)
array([[ 4.,  5.,  6.,  3.],
       [ 2.,  4.,  6.,  4.],
       [ 7.,  8.,  0.,  5.]])
\end{lstlisting}

What makes $LU$ decomposition so easy is that the inverses of
elementary matrices are elementary matrices.  For example, the
inverse of a Type 3 elementary matrix is the same matrix with the
opposite sign in the $(j,k)$ entry.  In the above problem, we have:
\begin{lstlisting}
>> L = type3m(3,2,1,A(2,1)/A(1,1)) * ...
       type3m(3,3,1,B(3,1)/B(1,1)) * ...
       type3m(3,3,2,C(3,2)/C(2,2))
\end{lstlisting}
Note that the minus signs are gone.  Doing it this way, we don't
have to actually invert anything to compute $L$.  This makes the
computation much faster.

\section*{Why Should I Care?}

The LU Decomposition isn't very useful when doing computations
by hand.  It is, however, very important in scientific computation
for the following reasons:
\begin{itemize}
\item If you want to solve the matrix equation $A x = b$, for several different vectors $b$, you can replace $A$ with $L$ and $U$, giving $L U x = b$.  Then solve the equations $L y = b$ and $U x = y$, using forward and backward substitution, respectively.  This is actually faster than solving them with row reduction.
\item The $LU$ decomposition allows quick computation of both inverses and determinants.
\item For very large matrices the $LU$ decomposition is crucial.  Indeed one can perform the $LU$ decomposition on a given matrix $A$ without needing additional space, that is, the program actually over-writes $A$ with $L$ and $U$.  Note that since the diagonal of $L$ are all ones, they don't need to be stored, and so the upper diagonal (including the diagonal) is $U$ and the lower diagonal (not including the diagonal) is $L$.
\end{itemize}

\begin{problem}\label{prob:lu}
Write a function which takes as input a random $n\times n$ matrix, performs the LU decomposition and returns $L$ and $U$.  To verify that it works, multiply $L$ and $U$ back together again and compare to $A$. Note: you should not use the {\tt la.inv()} function when you do this. You should only use the elementary matrices that we just created.
\end{problem}

\begin{problem}
Write a function which uses the solution to Problem \ref{prob:lu} to find the determinant of $A$.
\end{problem}
