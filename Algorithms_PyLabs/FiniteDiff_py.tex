\lab{Algorithms}{Numerical Derivatives}{Numerical Derivatives}

\objective{Learn a little about methods used to calculate derivatives numerically. Write code to approximate the Jacobian and the Hessian}

The derivative of a function at a point is formally defined as:

\[
f'(x) = \lim_{h\rightarrow 0} \frac{f(x + h)-f(x)}{h}
\]

In most real world applications we will be solving problems using computers. How does a computer calculate a limit? In short it can't. Computers can only approximate functions at specific points, and the notion of a limit graces infinity in a way that a computer never can.

So how can we use a computer to find the derivative of a function, particularly when we can't differentiate the function by hand? We use methods known as finite difference methods. For example suppose that in the previously formula instead of taking a limit we just pick a particularly small value for h. Then we can say:

\[
f'(x) \approx \frac{f(x + h)-f(x)}{h}
\]

This is known as the first order forward difference approximation of the derivative.

How do we know the quality of this approximation? We can use taylor's formula to find the following:

\[
f(x_0 + h) = f(x_0) + hf'(x_0) + h^2/2 f''(\xi),\hspace{5mm} \xi \in (x_0,x_0 + h)
\]

This can be manipulated to the following:

\[
f'(x_0) = \frac{f(x_0 + h) - f(x)}{h} + h/2 f''(\xi) = \frac{f(x_0 + h) - f(x)}{h} + O(h)
\]

Here we use the big O notation to denote that the errors are bounded by some constant times $h$.

We can use Taylor expansions to find approximations that have different big-O error bounds, up to any polynomial of arbitrary degree. The following tables offer the coefficients for forward, and centered difference schemes:

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
Derivative & Accuracy & -3 & -2 & -1 & 0 & 1 & 2 & 3 \\ \hline
 & 2 & & & -1/2 & 0 & 1/2 & & \\ \cline{2-9}
 1 & 4 & & 1/12 & -2/3 &  0 & 2/3 & -1/12 & \\ \cline{2-9}
  & 6 & -1/60 & 3/20 & -3/4 & 0 & 3/4 & -3/20 & 1/60 \\ \hline
  & 2 & & & 1 & -2 & 1 & & \\ \cline{2-9}
 2 & 4 & & -1/12 & 4/3 &  -5/2 & 4/3 & -1/12 & \\ \cline{2-9}
  & 6 & 1/90 & -3/20 & 3/2 & -49/18 & 3/2 & -3/20 & 1/90 \\ \hline
\end{tabular}
\caption{Centered Difference Coefficients}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Derivative & Accuracy & 0 & 1 & 2 & 3 & 4 \\ \hline
 & 1 & -1 & 1 &  & &  \\ \cline{2-7}
 1 & 2 & -3/2 & 2 & -1/2 & &  \\ \cline{2-7}
  & 3 & -11/6 & 3 & -3/2 & 1/3 &  \\ \hline
  & 1 & 1 & -2 & 1 &  & \\ \cline{2-7}
 2 & 2 & 2 & -5 & 4 &  -1 &  \\ \cline{2-7}
  & 3 & 35/12 & -26/3 & 19/2 & -14/3 & 11/12 \\ \hline
\end{tabular}
\caption{Forward Difference Coefficients}
\end{center}
\end{table}

These tables can be used by simply summing the function evaluations (the number at the top represents how many times $h$ is added to x), and then dividing by $h^n$, where $n$ is the degree of the derivative.

So, for example, the centered difference estimate of the second derivative that is $O(h^4)$ is:
\[
f''(x) \approx \frac{-1/12(f(x-2h) + f(x+2h)) + 4/3(f(x-h) + f(x+h)) -5/2f(x)}{h^2}
\]

Or, the forward difference estimate for the first derivative that is $O(h^2)$ is:

\[
f'(x) \approx \frac{-3/2f(x) + 2f(x+h) - 1/2 f(x+2h)}{h}
\]

It should be noted that we can convert a forward difference estimate to a backwards difference estimate by using a $-h$. So the backwards difference estimate for the first derivative that is $O(h^2)$ is:

\[
f'(x) \approx \frac{3/2f(x) - 2f(x-h) + 1/2 f(x-2h)}{h}
\]

There are two important observations that you should make about these tables. First, in order to get higher order approximations we need to evaluate the function at more points. This should not be surprising. Second, you should notice that centered difference formulas require less function evaluations to get higher order approximations. However, in certain applications it is not possible to use centered difference formulas, so the backwards and forwards formulas are still very applicable.

One important aspect of this method is selecting an appropriate $h$. The natural temptation is to pick a very very small value. However, this is not always advisable. Note the following table, which approximates the derivative of $e(x)$ at $x = 1$:

\begin{table}[h!]
\begin{center}
\begin{tabular}{|cc|}
\hline
h & Error  = $|f'(1)-f'_{app}(1)|$ \\ \hline
1e-1 & 4.5e-3 \\
1e-3 & 4.5305e-7 \\
1e-7 & 5.8587e-11 \\
1e-10 & 6.7274e-7 \\ \hline
\end{tabular}
\caption{Error in numerical derivative, using double floating point arithmetic}
\end{center}
\end{table}

As you can see, the error actually increases as $h$ becomes very small. Why is this? Division by small numbers causes errors in floating point arithmetic. So, be aware that usually the optimal $h$ is of moderately small size. However, in the framework of double floating point arithmetic this is usually less of a concern.

As a matter of reference, calculating numerical derivatives is an unstable operation. An unstable operation, informally, is one where errors are magnified by the operation. This usually is not an issue, but it's important to know that taking derivatives can amplify errors.

\begin{problem}
The Jacobian of a function $f:R^n \rightarrow R^m$ is an $m \times n$ matrix. It is defined by the formula:
\[
J_{i,j} = \frac{\partial f_i}{\partial x_j}
\]

Write a function {\tt Jacobian(j,n,m,h,o)} that accepts a function handle, the functions dimensions step size $h$ and option o. Based on the option have the function output the Jacobian using either a centered, forward or backward difference.

Test your function on the following:
\[
f(x) = 
\begin{pmatrix}
e^{x_1} sin(x_2) & x_2^3 \\
3x_2 & cos(x_1)
\end{pmatrix}
\] 

Test using the different options of your function. Also test using different values for $h$. How does the function perform (test against the analytically computed derivative)? Which method runs the fastest?
\end{problem}

\begin{problem}
The following formula can be used to compute a mixed partial derivative (note that $e_i$ is the unit vector with a one in the ith component). 
\small
\[
\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{f(x + (e_i + e_j)h) - f(x + (e_i-e_j)h) -f(x + (e_j-e_i)h) + f(x - (e_i + e_j)h)}{4h^2}
\]
\normalsize

The hessian is a matrix representing the mixed second partials of a function $f:\mathbb{R}^n \rightarrow \mathbb{R}$. This means that the hessian is a matrix defined by:

\[
H_{i,j} = \frac{\partial^2 f}{\partial x_i \partial x_j}
\]

Write a MATLAB function that numerically calculates the hessian of a given function.
\end{problem}
