\lab{Algorithms}{Eigenvalue Solvers}{Eigenvalue Solvers}
\label{Ch:EigSolve}

\objective{Understand how eigenvalue solvers work. Implement a simple eigenvalue solver.}

Consider the standard eigenvalue equation:

\[
A x_0 = \lambda x_0
\]

How can we find the eigenvalues? The standard approach in Linear Algebra textbooks is to note that:

\[
(A-\lambda I)x_0 = 0
\]

Or in other words that

\[
det(A-\lambda I) = 0
\]

This yields what is known as the characteristic polynomial:

\[
p(\lambda) = 0
\]

Finding the roots of the characteristic polynomials gives the eigenvalues of the matrix.

Unfortunately, exact root-finding is generally a difficult problem. In the case of matrices with complex entries we are looking for complex roots, which further complicates matters. The following theorem clearly articulates the difficulty of the problem:

\begin{theorem}
{\bf Abel's Theorem:} There are no general formulae solving polynomials of degree greater than four.
\end{theorem}

Accordingly, this means that there is no method that will exactly find the eigenvalues of an arbitrary matrix. This is a significant result (one of the few in algebra), and it means that we have to rely upon iterative methods: methods that converge to the eigenvalues. There are many such methods, but we will explore one of the simplest: the QR algorithm.

To explain we will start with a toy algorithm. Suppose we pick a random vector $x_0$. We can write $x_0$ as a linear combination of eigenvectors of $A$, namely:
\[
x_0 = \sum v_i
\]
Accordingly this means that we can write:
\[
Ax_0 = \sum \lambda_i v_i
\]

Now suppose that we examine $A^n x_0$. We can write the following:
\[
A^n x_0 = \sum \lambda_i^n v_i
\]

What happens as $n \rightarrow \infty$? It turns out that we end up converging to the eigenvector that corresponds to the largest eigenvalue of A (make sure that you understand why).

The only assumption that this algorithm makes is that the random vector is a linear combination of all of the eigenvectors (with non-zero coefficients). A random vector will satisfy this condition with probability one, so we generally do not have to worry about this condition. 

This toy algorithm only finds one eigenvector and value. However, by combining this algorithm with the Gram Schmidt process we can find all of the eigenvectors and values of a given matrix. This is what the QR algorithm does.

\section*{QR Algorithm}

The following recurrence describes the QR Algorithm:

\[
A_{k+1} = R_k Q_k, A_0 = A, A_k = Q_k R_k
\]

Where $Q_kR_k$ is the QR decomposition of $A_k$. Under certain conditions, this process will converge to an upper triangular matrix where the eigenvalues are on the diagonal.

\begin{problem}
Prove that $A_{k+1}$ is similar to $A_k$. This is central to the validity of this algorithm.
\end{problem}

\begin{matlab}
\begin{problem}
Code the QR algorithm as written above. Have your function accept a matrix and a number of iterations and return all the eigenvalues of the input matrix. You can use the QR function that you wrote earlier, or you can use MATLAB's. Use a random symmetric matrix to test your algorithm (compare your output to the output of {\tt eig}). How many iterations are necessary? How big a matrix does this function practically handle?
\end{problem}

\begin{problem}
Investigate your algorithm's performance on arbitrary matrices. What do you notice? How does your output differ from MATLAB's output? How is it the same?
\end{problem}
\end{matlab}

\begin{problem}
Have your function additionally output the eigenvectors corresponding to the eigenvalues. Hint: you have already made all the necessary calculations, you just need to store the information correctly.
\end{problem}

The QR algorithm is not the only iterative method used to find eigenvalues. For large sparse matrices MATLAB uses the Arnoldi iteration, which utilizes similar ideas as the QR algorithm while exploiting properties of the sparsity. Other methods include the Jacobi method and the Rayleigh quotient method.

It is important to remember that eigenvalue solvers can be wrong, particularly for matrices that are ill-conditioned. 
\begin{matlab}
For example using the MATLAB built in solver for the matrix generated by {\tt gallery(5)} yields the following output:
\begin{lstlisting}[style=matlab]
>> eig(gallery(5))
ans =
  -0.0405          
  -0.0118 + 0.0383i
  -0.0118 - 0.0383i
   0.0320 + 0.0228i
   0.0320 - 0.0228i
\end{lstlisting}

However, all of the eigenvalues of {\tt gallery(5)} are actually zero. This highlights the need to be conscious of the fact that eigenvalue solvers are really just doing an approximation.
\end{matlab}