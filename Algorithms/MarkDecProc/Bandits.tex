\lab{Algorithms}{Multi-Armed Bandit Problems}{Multi-Armed Bandit Problems}
\objective{This lesson explains Markov decision processes and specifically multi-armed bandit problems and solving them by Thompson Sampling.}

\section*{Markov Decision Processes}
Previously we considered what we called dynamic programming problems.  These problems involved making sequential decisions in some optimal way, possibly under uncertainty.  Dynamic programming problems are closely related to a class of problems called Markov Decision Processes.

A Markov Decision Process (MDP) involves the following elements

\begin{itemize}
\item   A set of decision times
\item   A set of states
\item   A set of actions
\item   A set of rewards dependent on the state and action
\item   Transition probabilities dependent on states and actions.
\end{itemize}

For our purposes we will consider discrete time problems so that our set of decision times is $t = 1,2,3,\ldots$.  In the dynamic programming problems considered in the previous labs, the set of states was the possible levels of wealth and the actions were how much to wealth to save for next period, the rewards were given by the utility function $u(\cdot)$, and we considered both deterministic and stochastic transitions.

\section*{Bandit Problems}
In particular, we will consider what is called the multi-armed bandit problem.  The name comes from the following example.  Suppose there is a row of $N$ slot machines (``one-armed bandits") that each pays out with probability $p_i$, $i= 1,2,\ldots,N$, where the probabilities are unknown to the gambler.  The gambler seeks to determine the sequence of levers to pull in order to maximize their winnings.

Bandit problems have a wide range of applications.  One can consider the ``arms" to be the different treatments in a clinical trial, the different forms of advertising a product, or the different research projects a company might invest in.

We now formulate the multi-armed bandit problem.  For simplicity we suppose there are $2$ arms, though the discussion is easily extendable to $N$ arms.  At each time $t= 1,2,\ldots$ one arm can be pulled.  With (unknown) probability $p_i$, the $i$th arm gives reward $1$ and with probability $1-p_i$ it gives reward $0$.  Note we have now defined a set of decision times and actions.  We define the state to be the number of successful and unsuccessful pulls on each arm written

\begin{equation}\label{state}
R(a_1,b_1,a_2,b_2)
\end{equation}
\noindent
where $a_i$ is the number of successful pulls on arm $i$ and $b_i$ is the number of unsuccessful pulls on arm $i$.

As we pull the arms, we must balance between pulling the arm that has the highest expected payoff and pulling all arms in order to gain information about the probabilities $p_i$.  This tradeoff is often referred to as exploring versus exploiting.  In essence, while gaining rewards, we will also come up with estimates of the $p_i$ that improve our decision making.  We will do so by Bayesian Updating.

\section*{Bayesian Updating}
While a full exposition of Bayesian inference is well beyond the scope of this lab, the essential concepts are fairly straightforward.  We recognize that we do not know the values of the $p_i$, but given our past history of successful and unsuccessful pulls we can say something about what range we think they might be in. This is different than guessing a specific value for the $p_i$.  For example the value of $p_i$ might be represented by a curve like seen in \ref{fig:priors}.

\begin{figure}
\begin{center}
        \includegraphics[scale=0.4]{./Algorithms/MarkDecProc/priors.pdf}
        \caption{Bayesian priors}
\end{center}
\label{fig:priors}
\end{figure}

The curves are thought of as probability distributions.  So if you have had $1$ success and $2$ failures, you might think that $p_i$ is around $\frac{1}{3} = \frac{1}{1+2}$. However, you have very little information at this point, so better yet you might represent your belief about $p_i$ by the blue probability distribution in Figure \ref{fig:priors}.  As you get more information, the curve gets narrower.  In the figure, we see that the distributions have expected value of $1/3$ and become tighter around that value.  This is fitting since the parameter values have success $\alpha$ one third of the time and the tighter distributions correspond to having more prior information.  If we need an estimate for $p_i$ we can use the expected value of our distribution corresponding to $p_i$.  We will denote this estimate by $\bar{p_i}$

In this manner we approach the multi-armed bandit problem.  We do not know the $p_i$ but each time we pull an arm, we update our distribution of where we think it might be.  In particular we use a beta distribution to represent our opinion on what the $p_i$ are.  A beta distribution has two parameters $a,b$.  So in our problem the state \eqref{state} can be thought of as representing two beta distributions one for each $p_i$. The details of the updating process are unimportant for now, except for this important property, the two parameters $a,b$ correspond exactly with successes and failures.  So if we start with a distribution Beta$(a,b)$ and the next pull is success, our new distribution is Beta$(a+1,b)$ and if the next pull is a failure then the new distribution is Beta$(a,b+1)$.  Notice this corresponds with the way our states evolve.

\section*{Simulation and Sampling in a Bayesian Framework}
Along with the estimate $\bar p_i$ TODO we can also compute many other useful quantities in our Bayesian problem via random sampling.  Essentially, we can take random draws from our prior distribution and estimate the mean, median, or other quantities based on the value of those quantities in the random sample.

TODO

\begin{problem}
Write a function sim\_data that accepts an $n\times 2$ array where each row represents the parameters of a beta distribution and a positive integer $k$ that represents the number of random draws to return.  The function should return an $n\times k$ matrix where each row has a random sample from each of the $k$ arms.
\end{problem}

\section*{Direct Dynamic Programming Solution}
This framework lends itself well to a dynamic programming type solutions.  Rather than just letting $R(a_1,b_1,a_2,b_2)$ represent the state, consider it as the value function, meaning the optimal expected value that can be achieved starting from this state.  Then we have

\begin{equation}\label{recurs}
\begin{aligned}
R(a_1,b_1,&a_2,b_2) =\\
 \max&\left\{\bar{p}_1\cdot[1 + \beta R(a_1+1,b_1,a_2,b_2)] + (1-\bar{p}_1)\beta R(a_1,b_1+1,a_2,b_2)\right. ,\\
&  \left.\bar{p}_2\cdot[1 + \beta R(a_1,b_1,a_2+1,b_2)] + (1-\bar{p}_2)\beta R(a_1,b_1,a_2,b_2+1)\right\}
\end{aligned}
\end{equation}

The two terms in the maximization represent the expected value of pulling lever one or lever two respectively.  For example, if lever one is pulled it is expected to yield a reward with probability $\bar{p_1}$.  The reward has value $1$ plus the future rewards (discounted by $\beta$) moving to a state with one more success on arm 1.  With probability $1-\bar{p_1}$, lever one does not give a reward and so rewards are simply the discounted expected reward starting in the next state.

Notice that the expressions inside of $R(\cdot)$ on the right side have parameters values that add to one greater than the $R(\cdot)$ on the left side.  So for example if we want to compute $R(1,1,1,1)$ we need to know $R(2,1,1,1)$, $R(1,2,1,1)$, $R(1,1,2,1)$ and $R(1,1,1,2)$ (all possible combinations of parameters that add up to 5).  To compute these, we need to know all possible combinations of parameters that add up to 6, and so on.  Consequently, we could make a guess for all $R$ for all parameter combinations that add up to some large $N$, then work backward until we get to $R(1,1,1,1)$.  This is backward induction, just as we saw in dynamic programming.  However, the number of computations in this problem grow much too quickly because of the branching nature of having multiple arms.  In fact, if there are more than two arms this method of computation is infeasible!


\begin{comment}
\section*{Gittins Index Solution}
One way we might hope to solve a bandit problem is by computing some sort of ``index'' for each arm.  That is, we want a number associated with each arm that in some sense captures the value of pulling that arm.   Ideally such an index would depend only on that arm, and not on the others.  We could then compare the indices for all of the arms and pull the arm with highest index.  It turns out that bandit problems can be solved optimally by such methods and are computationally more feasible than the dynamic programming approach we saw above.

To compute an index for this problem, we consider comparing an arm with unknown payoff probability $p_i$ to an arm with known payoff probability $p$.  Then equation \eqref{recurs} becomes

\begin{equation}\label{index}
\begin{aligned}
R(p,a_i,b_i) = \max&\left\{\frac{p}{1-\beta} \right. ,\\
&  \left.\hat{p}_i\cdot[1 + \beta R(p,a_i+1,b_i)] + (1-\hat{p}_i)\beta R(p, a_i,b_i+1)\right\}.\\
\end{aligned}
\end{equation}

We determine the expected value of pulling the first arm to be $\frac{p}{1-\beta}$ noting that if we pull the deterministic arm once, we will continue pulling it forever as we gain no new information.  In this case the expected reward from pulling the known arm is $p + \beta p + \beta^2 p + \cdots = \frac{p}{1-\beta}$.


If we can find the $p$ such that we are indifferent between the deterministic arm and the unknown arm, this will give us an index that quantifies the value of arm $p_i$ (this can be proved).

Putting all of this together, our algorithm for solving the multi-armed bandit problem with two arms is as follows.  For each arm $i$, compute \eqref{recurs} over a range of $p$ values to find the $p$ such that you would be indifferent between the arm with probability $p$ and arm $i$. Store this as the index $\lambda_i$.  Compare the $\lambda_i$ and pull the arm with largest index.

In order to compute \eqref{index} we use dynamic programming, starting with a guess for $R$ for parameters that add up to some large $N$ and use backward induction.  In the process, we find $R$ and $\lambda_i$ for each combination of parameters that adds up to any $n\leq N$. Thus we do not have to compute new $\lambda_i$ after each pull, we can just look them up.

\section*{Algorithm Outline}
This section will guide you through creating a function that will compute the indices for a given arm.  It will involve writing a number of functions that you can save in the same .py file.

First, we need a function that will compute all the pairs of $a,b$ that add up to some $N$.  We also want the user to be able to input the minimum values of $a$ and $b$ of interest to avoid unnecessary computation.  For example if in practice all of the arms have $a_i \geq 5$ and $b_i \geq 10$, then we are uninterested in smaller $a$, $b$.  The following code accepts the value of $N$ and a minimum value of a and b and returns an $N$ by 2 array with the $a$'s and $b$'s such that $a + b = N$.

\begin{lstlisting}[style = python]

# computes pairs of numbers starting with mina and minb that
# add up to N

def compute_indices(N,mina,minb):
    import scipy as sp
    avec = sp.arange(mina,N-minb+1)
    avec = sp.reshape(avec,(avec.shape[0],1))
    bvec  = sp.arange(N-mina, minb-1,-1)
    bvec = sp.reshape(bvec,(bvec.shape[0],1))
    values = sp.hstack((avec,bvec))

    return values
\end{lstlisting}

In order to perform the backward induction, we need to be able to estimate the value $R(p,a,b)$ for $a+b = N$.  To do so we have to estimate the second quantity in \eqref{index}.  We will estimate it as $(\frac{a}{a+b})/(1-\beta)$.  This is the value one would get by pulling an arm with $p_i$ equal to the expected value of $Beta(a,b)$ forever.
\begin{problem}
Write a function ``end\_reward" that accepts $p,a,b,$ and $\beta$ and returns the estimated value of $R(p,a,b)$ for $a,b$ such that $a+b=N$.
\end{problem}

For convenience it will be nice to have a function that will compute $R(p,a,b)$ given values of $p,\bar{p},a,b,\beta,N$ as well as values of $R(p,a+1,b)$ and $R(p,a,b+1)$.  This should follow directly from \eqref{index}.

\end{comment}

\section*{Thompson Sampling}
There is a method for computing the optimal solution to the multi-armed bandit problem called the Gittins Index Theorem.  Computationally it is similar to the dynamic programming approach, but less costly.  However, it is still too costly for large problems.

There are, however, many heuristic methods of solving the multi-armed bandit problem.  In particular we will use a method sometimes called Thompson Sampling, or Randomized Probability Matching.  The idea is that we should choose arm $i$ with probability equal to the...


\begin{comment}
\begin{problem}
Write a function ``rewards" that will compute $R(p,a,b)$ given the quantities mentioned above.
\end{problem}

With these functions in hand we are ready to perform the backward induction.  Our function should accept $p,a,b,\beta,$ and $N$ and return the value of $R(p,a,b)$.  We will use the values of $R(p,a,b)$ for $a+b = N$ determined from our previous functions as a starting point for our backward induction.  Although not the most efficient, for simplicity, we will store $R(p,a,b)$  as the $a,b$ entry of a matrix.

\begin{problem}
Write a function ``backward" as described above by following the steps below.  This should be the only function in this lab that requires more than just a few lines.

\begin{enumerate}
\item   Initialization: First use the functions written previously to obtain an $N\times 2$ array of indices $a,b$ that add up to $N$ along with an $N\times 1$ vector of the rewards associated with each of those $a,b$ pairs.  Initialize the $N\times N$ $R$ matrix to zeros.

    Now initialize the entries of $R$ for all the entries corresponding to $a+b = N$ with the rewards determined previously.

\item   Induction: Iterate backward to fill in the $R$ matrix.  Use the ``rewards" function to compute the value of $R(p,a,b)$ given $R(p,a+1,b)$ and $R(p,a,b+1)$ as you go.  Note you will have to be careful about the order in which you iterate through the matrix so that you only fill entries $a,b$ for which $R(a+1,b)$ and $R(a,b+1)$ have already been computed.  Return the $R$ matrix.
\end{enumerate}
\end{problem}

Now, we need to compute the $R$ matrix for many values of $p$ in order to determine the index.

\begin{problem}
Write a function ``compute\_pgrid" that accepts a minimum value of $a$ and of $b$, $\beta$, and $N$ and returns two things.  First it should return a $100\times N\times N$ array that consists of 100 $R$ matrices given by the ``backward" function each corresponding to a different value of $p$ where the values of $p$ are evenly spaced between 0 and 100.  Also return the vector of $p$ values.
\end{problem}

Now all of the pieces are in place to compute the $\lambda_i$ for a given arm.

\begin{problem}
Write a function ``compute\_gittins" that accepts $a,b,\beta$, a vector of $p$ values, and the corresponding $100\times N\times N$ array of $R(p,a,b)$ from the ``compute\_pgrid" function.  Use these to find the value of $p$ such that for an arm with $a$ successes and $b$ failures, you would be indifferent between pulling that arm and an arm with known probability $p$.  Remember, the reward for pulling the known arm is $\frac{p}{1-\beta}$.  That value of $p$ is the index $\lambda$.
\end{problem} 
\end{comment}\

With these functions in hand we can easily compute which arm to pull at each time step as follows:

In the following applications lab, we will investigate an area in which this is useful and simulate the bandit problem using the functions in this lab.