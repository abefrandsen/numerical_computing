\lab{Algorithms}{Nested and Lambdad Functions}{Nested and Lambda Functions}

\objective{Learn about nested and lambda functions}

\section*{Nested functions}
So far all the functions we have been defining are called \emph{global} functions.  This means that these functions are accessible to any program that imports your module.  In Python, it is possible to define nested functions (functions inside of functions).  Nested functions are called \emph{local} functions.  They have access to all the variables that the parent function has, but they are not accessible outside of the parent function.  Consider an example where you reuse a specific block of code many times inside a single function.  This block of code would be a good candidate for a nested function.  Nested functions, and functions in general, must be defined before they are called. Realize that nested functions do have overhead, so only use them when necessary.  Here's an example of pseudocode for Newton's Method:

\begin{lstlisting}
def NewtonsMethod(f,x0):   
    def derivative(f):
        #Our nested function is defined first.
        #It is usally good practice to define
        #your functions as early as possible

        #calculate derivative of f
        return derivative
    
    #Perform Newton's Method
    #we need the deriviative of f
    df = derivative()

    #more calculations
    return roots
\end{lstlisting}

Our nested function \li{derivative} has access to the variables \li{f} and \li{x0}.  Even though we have access to \li{f}, it would still be a good idea to define our nested to function to take one argument, \li{f}.  Our nested function \li{derivative} however, is not accessible outside of \li{NewtonsMethod}.

\section*{Lambda Functions}
A lambda function allows us to create simple, one expression functions. The syntax is illustrated by this example.  Lambda functions are defined as \emph{lambda} functions.

\begin{lstlisting}
: from math import sin, cos
: sincos = lambda x: sin(x) + cos(x)
\end{lstlisting}

Now we can find the value of $\sin(x) + \cos(x)$ by simply typing \li{sincos(x)}.

Oftentimes in real applications we are attempting to do operations on a function. Examples would include root finding, minimization, integration or solving differential equations. Python allows you to pass entire functions as arguments to another function.  If you recall in the beginning of this book, our timing context is a function that accepts another function as input.  Define the following function in Python.
\begin{lstlisting}
: def myFunc(a,b):
....: return a**b
: myFunc
<function myFunc at 0x8b33f44>
: m = lambda x, y: x**y
: m
<function <lambda> at 0x8b45534>
\end{lstlisting}

Notice how lambda functions are much more compact. We are essentially defining the lambda function on-the-fly.  Note that lambda functions cannot have a return statement and must be a single line (even though it does have a colon).
\begin{lstlisting}
: from timer import timer
: with timer(loops=1000) as t:
....: t.timer(myFunc, 3,5)
....: t.timer(lambda x,y: x**y, 3,5)
\end{lstlisting}

Let's look at another example of a function that accepts a function as an argument.
\begin{lstlisting}
: from scipy import integrate
: integrate.quad(lambda x: sp.exp(-x**2/2.0), 0, 2)
(1.1962880133226084, 1.3281464964738456e-14)
\end{lstlisting}

This code will calculate the integral of $e^{-\frac{x^2}{2}}$ between zero and two. Here we used a lambda function to get the job done. 

\begin{problem}
Write a function that accepts a number $n$, creates a random polynomial of degree $n$ and then finds a zero of that polynomial. If there is no zero then have the function return a local minimum instead. You will need to create function handles and use the function \li{scipy.optimize.bisect}.
\end{problem}

\begin{problem}
The Lambert W-function is defined as the inverse of the function $f(x) = xe^x$. The W-function has important applications in atomic physics and differential equations. It has no closed-form solution. Create a function that estimates the Lambert W-function (you'll need to use \li{bisect} again).
\end{problem}

\begin{problem}
Read the help file on the function \li{scipy.integrate.odeint}. Use \li{odeint} to find the solution of the differential equation
\begin{equation*}
        x'' = -3x \quad x(0) = 1 \quad x'(0) = 0
\end{equation*}

To do this, you will want to convert the higher order system into a lower order system of more variables. Try letting $x_1 = x$ and $x_2 = x'$.
Now plot the output. What do you find? Is this what you expect?
\end{problem}

\begin{problem}
Newton's method is an iterative root-finding method. It generally converges very quickly, and is very important in non-linear optimization. Newton's method starts with an initial guess $x_0$ for a root and then calculates the next guess using the following method:
\begin{equation*}
x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}
\end{equation*}

This procedure repeats until a tolerance is reached (either $x_i - x_{i+1}$ or $f(x_{i+1})$ is small enough)
One of the important advantages to Newton's method as opposed to functions like \li{bisect} is that they are more robust when functions have asymptotes (try to use \li{bisect} on \li{tan} to see this). Also, Newton's method is generally much faster, and is generalizable to higher dimensions.
Write your own Newton's method. Have it accept as arguments f, f', $x_0$ and tol. Remember that f and f' (which you'll probably have to name something like df or fPrime) will be function handles that are passed in. Hint: Use a while loop that tests for reaching the tolerances.
\end{problem}
