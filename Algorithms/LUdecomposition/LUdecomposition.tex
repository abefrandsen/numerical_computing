\lab{Algorithms}{RREF/Elementary Matrices}{RREF/Elementary Matrices}
\label{lab:LUdecomp}
\objective{In this section we will use elementary matrices to find the RREF and to find the LU decomposition.}

In Linear algebra there are 3 elementary row operations: switching two rows, multiplying a row by a constant, and adding a multiple of one row to another row.
We carry out each of these operations with a corresponding elementary matrix.
These matrices are easy to construct.
Suppose $A$ is an $m \times n$ matrix and you want to perform one of the three elementary operations on $A$.
You can do this be constructing the $m \times m$ identity matrix, $I$, performing the elementary row operation on $I$ to obtain $E$ and then multiplying $EA$.
For example, consider the matrix
\[
A = \begin{pmatrix}
a_{11}&a_{12}&a_{13}&a_{14}\\
a_{21}&a_{22}&a_{23}&a_{24}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}
\]
If we want to swap the first two rows, we can left multiply the
matrix $A$ by:
\[
E = \begin{pmatrix}
0&1&0\\
1&0&0\\
0&0&1
\end{pmatrix},
\]
then
\[
E A =
\begin{pmatrix}
a_{21}&a_{22}&a_{23}&a_{24}\\
a_{11}&a_{12}&a_{13}&a_{14}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}.
\]
E in this case is called a type I matrix.

Now let's examine the next row operation.
If we want to multiply the second row of $A$ by the constant $b$, we can left multiply the matrix $A$ by the following matrix:
\[
\tilde{E} = \begin{pmatrix}
1&0&0\\
0&b&0\\
0&0&1
\end{pmatrix}.
\]
Then
\[
\tilde{E} A =
\begin{pmatrix}
a_{11}&a_{12}&a_{13}&a_{14}\\
b a_{21}&b a_{22}&b a_{23}&b a_{24}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}.
\]
$\tilde{E}$ is called a type II matrix.  

Now let's examine the last row operation.
If we want to multiply, say, the first row of $A$ by a constant $c$ and add it to the second row, we can left multiply the matrix $A$ by the following matrix:
\[
\widehat{E} = \begin{pmatrix}
1&0&0\\
c&1&0\\
0&0&1
\end{pmatrix}.
\]
Then
\[
\widehat{E} A =
\begin{pmatrix}
a_{11}&a_{12}&a_{13}&a_{14}\\
c a_{11} + a_{21}&c a_{12} + a_{22}&c a_{13} + a_{23}&c a_{14} + a_{24}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}.
\]
$\widehat{E}$ is called a type III matrix.

These elementary matrices are useful when considering matrices in a theoretical context.
In practice, matrix multiplication is much more expensive than directly performing row operations, so when implementing elementary row operations we will modify arrays directly.

The following is some Python code that generates elementary matrices to perform specific row operations.
\lstinputlisting[style=python,name=]{row_opers.py}

\section*{Programming Row Reduction}

Solving a linear system can be done most efficiently by using elementary row operations to reduce a matrix to ``row echelon form'' (REF), as opposed to ``reduced row echelon form'' (RREF).
Consider the following matrix: 

\[
\begin{pmatrix}
4&5&6&3 \\
2&4&6&4 \\
7&8&0&5
\end{pmatrix}
\]

Using elementary row operations, we can reduce as follows:
\begin{lstlisting}
: import scipy as sp
: import row_opers as op
: A = sp.array([[4., 5., 6., 3.],[2., 4., 6., 4.],[7., 8., 0., 5.]])
array([[ 4.,  5.,  6.,  3.],
       [ 2.,  4.,  6.,  4.],
       [ 7.,  8.,  0.,  5.]])
: cmultadd(A, 1, 0, -A[1,0]/A[0,0])
: cmultadd(A, 2, 0, -A[2,0]/A[0,0])
: cmultadd(A, 2, 1, -A[2,1]/A[1,1])
: A
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 0. ,  0. , -9. ,  1. ]])
\end{lstlisting}

To finish finding the REF we would need to divide each row by its leading coefficient.

\begin{problem}
\label{prob:REF}
Write a Python function, which takes an $n\times (n+1)$ matrix (in other words and \emph{augmented} matrix) as input and reduces it to REF using naive row reduction.
Assume that the matrix is invertible and ignore the possibility that a zero may appear on the main diagonal during row reduction.
\end{problem}

\section*{LU Decomposition}

Using row reduction we can reduce an invertible matrix $A$ to upper triangular form.
Say this can be done in $k$ row operations.
Let $U$ be the upper triangular form of $A$, so we have:
Hence, we have
\[
U = E_k \dots E_2 E_1 A.
\]
Since the elementary matrices are invertible, we also have
\[
(E_k \dots E_2 E_1)^{-1} U =  A.
\]
Then we define $L$ to be
\[
L = (E_k \dots E_2 E_1)^{-1}
\]
which is the same as
\[
L = E_1^{-1} E_2^{-1} \dots E_k^{-1}
\]
In Either case, we have $L U = A$.

What makes $LU$ decomposition so nice to work with is that the inverses of elementary matrices are elementary matrices.
For example, the inverse of a Type 3 elementary matrix is the same matrix with the opposite sign in the $(j,k)$ entry.
Doing it this way, we don't have to actually invert anything to compute $L$.
This makes the computation much faster.

\section*{Why This Matters}

The LU decomposition is more efficient for solving linear systems than traditional row reduction.
It also can be applied to matrices with more than one column.
Say you want to solve the matrix equation $A x = b$, for several different $b's$, you can replace $A$ with $L$ and $U$, giving $L U x = b$.
The $LU$ decomposition also allows quick computation of both inverses and determinants.
For very large matrices, the LU decomposition can be performed without using any extra space.
We can store L in the poriton of the array that lies below the main diagonal and U from the main diagonal up.
We don't have to store the main diagonal of L since all its entries are ones.

\begin{problem}
\label{prob:LU}
Write a Python function which takes as input a random $n\times n$ matrix, performs the LU decomposition and returns $L$ and $U$.
To verify that it works, multiply $L$ and $U$ together and compare to $A$.
Assume that the matrix is invertible and ignore the possibility that a zero may appear on the main diagonal during row reduction.
Note: you should not use the \li{inv} function when you do this.
\end{problem}

\begin{problem}
\label{prob:det}
Write a Python function which uses the solution to problem \ref{prob:REF} to find the determinant of $A$.
Notice that in problem \ref{prob:REF} you really found $U$.
\end{problem}
