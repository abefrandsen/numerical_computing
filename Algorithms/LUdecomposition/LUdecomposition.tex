\lab{Algorithms}{LU Decomposition}{LU Decomposition}
\label{lab:LUdecomp}
\objective{In this section we will find the REF and the LU decomposition.}

In Linear algebra there are 3 elementary row operations: switching two rows, multiplying a row by a constant, and adding a multiple of one row to another row.
Each of these operations can, in theory, be done by left multiplication by a corresponding elementary matrix.
This approach is \emph{extremely} slow in practice.
In practice, it is much faster to perform these operations directly by modifying only the portions of an array that change as a result of the row operation.

These sorts of operations are relatively easy to apply.
The first sort of operation is swapping rows.
The second sort of operation is multiplying a row by a constant.
The third sort of operation is adding a constant multiple of one row to another.
Theoretically, each of these operations can be represented as multiplication by an elementary matrix.

The following is some Python code that generates elementary matrices to perform specific row operations.
\lstinputlisting[style=python,name=]{row_opers.py}

\section*{Programming Row Reduction}

Solving a linear system can be done most efficiently by using elementary row operations to reduce a matrix to ``row echelon form'' (REF), as opposed to ``reduced row echelon form'' (RREF).
Consider the following matrix: 

\[
\begin{pmatrix}
4&5&6&3 \\
2&4&6&4 \\
7&8&0&5
\end{pmatrix}
\]

Using elementary row operations, we can reduce as follows:
\begin{lstlisting}
: import numpy as np
: A = np.array([[4., 5., 6., 3.],[2., 4., 6., 4.],[7., 8., 0., 5.]])
array([[ 4.,  5.,  6.,  3.],
       [ 2.,  4.,  6.,  4.],
       [ 7.,  8.,  0.,  5.]])
: A[1] -= (A[1,0]/A[0,0]) * A[0]
: A[2] -= (A[2,0]/A[0,0]) * A[0]
: A[2,1:] -= (A[2,1]/A[1,1]) * A[1,1:]
: A
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 0. ,  0. , -9. ,  1. ]])
\end{lstlisting}

To finish finding the REF we would need to divide each row by its leading coefficient.
Notice that in our third row operation we were able to operate on only a portion of the third row because we knew that the first value would still be 0.
In this case it made little difference, but it is a good idea to watch for things like this because they can save a great deal more time when we are working with larger matrices.

\begin{problem}
\label{prob:REF}
Write a Python function, which takes an $n\times (n+1)$ matrix (in other words and \emph{augmented} matrix) as input and reduces it to REF using naive row reduction.
Assume that the matrix is invertible and ignore the possibility that a zero may appear on the main diagonal during row reduction.
\end{problem}

\section*{LU Decomposition}

Using row reduction we can reduce an invertible matrix $A$ to upper triangular form.
Say this can be done in $k$ row operations.
Let $U$ be the upper triangular form of $A$, so we have:
Hence, we have
\[
U = E_k \dots E_2 E_1 A.
\]
Since the elementary matrices are invertible, we also have
\[
(E_k \dots E_2 E_1)^{-1} U =  A.
\]
Then we define $L$ to be
\[
L = (E_k \dots E_2 E_1)^{-1}
\]
which is the same as
\[
L = E_1^{-1} E_2^{-1} \dots E_k^{-1}
\]
In Either case, we have $L U = A$.

What makes $LU$ decomposition so nice to work with is that the inverses of elementary matrices are elementary matrices.
This means that we can compute $L$ by performing a series of simple row operations on an identity matrix.
As it turns out, when we are only doing type 3 row operations, each of the operations represented by right multiplication by these inverse matrices results in a change in a single entry of $L$.
This makes the computation much faster.

In practice, the LU decomposition of an array $A$ can be computed like this:
\begin{itemize}
\item Make a copy $U$ of $A$.
\item Make an identity matrix $L$ of the same shape as $A$.
\item Iterate through the entries below the diagonal of $U$ like you would when performing row reduction.
For each entry below the main diagonal of $U$ do the following:
	\begin{itemize}
	\item Set the corresponding entry of $L$ to the quotient of the current entry of $U$ and the entry of the main diagonal of $U$ lying above the current entry.
	\item Perform the type three row operation to set the current entry of $U$ to 0.
		Remember to avoid computation involving columns you have already finished processing.
	\end{itemize}
\item Return L and U.
\end{itemize}

\section*{Why This Matters}

The LU decomposition is more efficient for solving linear systems than traditional row reduction.
It also can be applied to matrices with more than one column.
Say you want to solve the matrix equation $A x = b$, for several different $b's$, you can replace $A$ with $L$ and $U$, giving $L U x = b$.
The $LU$ decomposition also allows quick computation of both inverses and determinants.
For very large matrices, the LU decomposition can be performed without using any extra space.
We can store L in the poriton of the array that lies below the main diagonal and U from the main diagonal up.
We don't have to store the main diagonal of L since all its entries are ones.

\begin{problem}
\label{prob:LU}
Write a Python function which takes as input a random $n\times n$ matrix, performs the LU decomposition and returns $L$ and $U$.
To verify that it works, multiply $L$ and $U$ together and compare to $A$.
Assume that the matrix is invertible and ignore the possibility that a zero may appear on the main diagonal during row reduction.
Note: you should not use the \li{inv} function when you do this.
\end{problem}

\begin{problem}
\label{prob:det}
Write a Python function which uses the solution to problem \ref{prob:REF} to find the determinant of $A$.
Notice that problem \ref{prob:REF} computes $U$ when applied to a square matrix.
\end{problem}
