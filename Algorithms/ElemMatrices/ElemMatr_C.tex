\lab{Algorithms}{RREF/Elementary Matrices}{RREF/Elementary Matrices}
\label{lab:LUdecomp}
\objective{In this section we will use elementary matrices to find the RREF and to find the LU decomposition.}

In Linear algebra there are 3 elementary row operations: switching two rows, multiplying a row by a constant, and adding a multiple of one row to another row.
We carry out each of these operations with a corresponding elementary matrix.
These matrices are easy to construct.
Suppose $A$ is an $m \times n$ matrix and you want to perform one of the three elementary operations on $A$.
You can do this be constructing the $m \times m$ identity matrix, $I$, performing the elementary row operation on $I$ to obtain $E$ and then multiplying $EA$.
For example, consider the matrix
\[
A = \begin{pmatrix}
a_{11}&a_{12}&a_{13}&a_{14}\\
a_{21}&a_{22}&a_{23}&a_{24}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}
\]
If we want to swap the first two rows, we can left multiply the
matrix $A$ by:
\[
E = \begin{pmatrix}
0&1&0\\
1&0&0\\
0&0&1
\end{pmatrix},
\]
then
\[
E A =
\begin{pmatrix}
a_{21}&a_{22}&a_{23}&a_{24}\\
a_{11}&a_{12}&a_{13}&a_{14}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}.
\]
E in this case is called a type I matrix.

Now let's examine the next row operation.
If we want to multiply the second row of $A$ by the constant $b$, we can left multiply the matrix $A$ by the following matrix:
\[
\tilde{E} = \begin{pmatrix}
1&0&0\\
0&b&0\\
0&0&1
\end{pmatrix}.
\]
Then
\[
\tilde{E} A =
\begin{pmatrix}
a_{11}&a_{12}&a_{13}&a_{14}\\
b a_{21}&b a_{22}&b a_{23}&b a_{24}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}.
\]
$\tilde{E}$ is called a type II matrix.  

Now let's examine the last row operation.
If we want to multiply, say, the first row of $A$ by a constant $c$ and add it to the second row, we can left multiply the matrix $A$ by the following matrix:
\[
\widehat{E} = \begin{pmatrix}
1&0&0\\
c&1&0\\
0&0&1
\end{pmatrix}.
\]
Then
\[
\widehat{E} A =
\begin{pmatrix}
a_{11}&a_{12}&a_{13}&a_{14}\\
c a_{11} + a_{21}&c a_{12} + a_{22}&c a_{13} + a_{23}&c a_{14} + a_{24}\\
a_{31}&a_{32}&a_{33}&a_{34}
\end{pmatrix}.
\]
$\widehat{E}$ is called a type III matrix.

The following is some Python code that generates elementary matrices to perform specific row operations.
\lstinputlisting[style=python,name=]{row_opers.py}

\section*{Programming Row Reduction}

Solving a linear system can be done most efficiently by using elementary row operations to reduce a matrix into ``row echelon form'' (REF), as opposed to ``reduced row echelon form'' (RREF).
Consider the following matrix: 

\[
\begin{pmatrix}
4&5&6&3 \\
2&4&6&4 \\
7&8&0&5
\end{pmatrix}
\]

By left multiplying by elementary matrices, we can reduce as follows:
\begin{lstlisting}
: import scipy as sp
: import row_opers as op
: A = sp.array([[4, 5, 6, 3],[2, 4, 6, 4],[7, 8, 0, 5]], dtype='float32')
array([[ 4.,  5.,  6.,  3.],
       [ 2.,  4.,  6.,  4.],
       [ 7.,  8.,  0.,  5.]], dtype=float32)
: A1 = sp.dot(op.cmultadd(3,1,0,-A[1,0]/A[0,0]), A); A1
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 7. ,  8. ,  0. ,  5. ]])
: A2 = sp.dot(op.cmultadd(3,2,0,-A1[2,0]/A1[0,0]), A1); A2
array([[  4.  ,   5.  ,   6.  ,   3.  ],
       [  0.  ,   1.5 ,   3.  ,   2.5 ],
       [  0.  ,  -0.75, -10.5 ,  -0.25]])
: A3 = sp.dot(op.cmultadd(3,2,1,-A2[2,1]/A2[1,1]), A2); A3
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 0. ,  0. , -9. ,  1. ]])
\end{lstlisting}

To finish finding the REF we would need to divide each row by its leading coefficient.
That can be done using Type II matrices.

\begin{problem}
\label{prob:REF}
Write a Python function, which takes an $n\times (n+1)$ matrix (in other words and \emph{augmented} matrix) as input and performs the above naive row reduction to REF using elementary matrices.
Assume that the matrix is invertible and ignore the possibility that a zero may appear on the main diagonal during row reduction.
\end{problem}

\section*{LU Decomposition}

Again, consider the matrix $A$. By iteratively left multiplying by Type 3 elementary matrices, we reduce as follows:

\begin{lstlisting}
: E1 = op.cmultadd(3,1,0,-A[1,0]/A[0,0]); E1
array([[ 1. ,  0. ,  0. ],
       [-0.5,  1. ,  0. ],
       [ 0. ,  0. ,  1. ]])
: B1 = sp.dot(E1, A)
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 7. ,  8. ,  0. ,  5. ]])
: E2 = op.cmultadd(3,2,0,-B1[2,0]/B1[0,0])
: B2 = sp.dot(E2, B1)
: E3 = op.cmultadd(3,2,1,-B2[2,1]/B2[1,1])
: U = sp.dot(E3, B2); U
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 0. ,  0. , -9. ,  1. ]])
\end{lstlisting}
Note that we have reduced the above matrix into upper-triangular form, denoted as $U$.
Hence, we have
\[
U = E_3 E_2 E_1 A.
\]
Since the elementary matrices are invertible, we also have
\[
(E_3 E_2 E_1)^{-1} U =  A.
\]
This can be re-written as
\[
E_1^{-1} E_2^{-1} E_3^{-1} U =  A.
\]
Then we define $L$ to be
\[
L = E_1^{-1} E_2^{-1} E_3^{-1},
\]
which yields $L U = A$.  
\begin{lstlisting}
: from scipy import linalg as la
: I = lambda x: la.inv(x)
: L = sp.dot(sp.dot(I(E1), I(E2)), I(E3)); L
array([[ 1.  ,  0.  ,  0.  ],
       [ 0.5 ,  1.  ,  0.  ],
       [ 1.75, -0.5 ,  1.  ]])
: sp.dot(L, U)
array([[ 4.,  5.,  6.,  3.],
       [ 2.,  4.,  6.,  4.],
       [ 7.,  8.,  0.,  5.]])
\end{lstlisting}
What makes $LU$ decomposition so easy is that the inverses of elementary matrices are elementary matrices.
For example, the inverse of a Type 3 elementary matrix is the same matrix with the opposite sign in the $(j,k)$ entry.
In the above problem, we have:
Note that the minus signs are gone.
Doing it this way, we don't have to actually invert anything to compute $L$.
This makes the computation much faster.

\section*{Why This Matters}

The LU decomposition is more efficient for solving linear systems than traditional row reduction.
\begin{itemize}
\item If you want to solve the matrix equation $A x = b$, for several different $b's$, you can replace $A$ with $L$ and $U$, giving $L U x = b$.
Then solve the equations $L y = b$ and $U x = y$, using forward and backward substitution, respectively.
This is actually faster than solving them with row reduction.
\item The $LU$ decomposition allows quick computation of both inverses and determinants.
\item For very large matrices the $LU$ decomposition is crucial.
Indeed one can perform the $LU$ decomposition on a given matrix $A$ without needing additional space, that is, the program actually over-writes $A$ with $L$ and $U$.
Note that since the diagonal of $L$ are all ones, they don't need to be stored, and so the upper diagonal (including the diagonal) is $U$ and the lower diagonal (not including the diagonal) is $L$.
\end{itemize}

\begin{problem}
\label{prob:LU}
Write a Python function which takes as input a random $n\times n$ matrix, performs the LU decomposition and returns $L$ and $U$.
To verify that it works, multiply $L$ and $U$ together and compare to $A$.
Assume that the matrix is invertible and ignore the possibility that a zero may appear on the main diagonal during row reduction.
Note: you should not use the \li{inv} function when you do this.
\end{problem}

\begin{problem}
\label{prob:det}
Write a Python function which uses the solution to Problem \ref{prob:LU} to find the determinant of $A$.
\end{problem}
