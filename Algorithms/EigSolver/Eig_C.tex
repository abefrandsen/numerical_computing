\lab{Algorithms}{Eigenvalue Solvers}{Eigenvalue Solvers}
\label{Ch:EigSolve}

\objective{Implement the $QR$ algorithm for finding eigenvalues.}

Finding the eigenvalues of $n \times n$ matrix $A$ means solving the following equation, where $x$ is a nonzero vector and $\lambda$ is a scalar.
\begin{align} 
 A x                       &=  \lambda x  \notag \\
A x - \lambda x &= 0 \notag \\
(A - \lambda I)x  &= 0 \label{eq:singularity}
\end{align}
Since $x$ is nonzero, \eqref{eq:singularity} means $A-\lambda I$ must be singular. Thus $det(A-\lambda I) = 0$.  This determinant is often notated $det(A-\lambda I) = p(\lambda)$ and is called the \emph{characteristic polynomial} of $A$. Finding the roots of the characteristic polynomial gives the eigenvalues of $A$.

If $A$ is $n \times n$, the degree of $p(\lambda) = n$.   For small $n$, finding the eigenvalues is not difficult.  However, as the size of $A$ increases, finding the roots of $p(\lambda)$ becomes difficult.  Abel's Theorem  outlines the problem.

\begin{theorem}
\label{Theorem:Abel}
{\bf Abel's Impossibility Theorem:} There is no general algebraic solution for solving a polynomial equation of degree $n>4$.
\end{theorem}

Therefore, there is no method that will exactly find the eigenvalues of an arbitrary matrix. This is a significant result. In practice it means that we often rely on iterative methods, which converge to the eigenvalues. There are many such methods, but we will explore one of the simplest: the $QR$ algorithm. The following recurrence describes the $QR$ Algorithm in its most basic form. 

\begin{equation*}
A_0 = A, \quad A_k = Q_k R_k, \quad A_{k+1} = R_k Q_k
\end{equation*}

where $Q_k, R_k$ is the $QR$ decomposition of $A_k$. Yes, it's as easy as it looks. All this algorithm does at each step is find the $QR$ decomposition of $A_k$ and multiply $Q_k$ and $R_k$ together again but in the opposite order. How does this simple process find the eigenvalues of $A$? 

\begin{problem}
\label{problem:similarity proof}
Prove that $A_{k+1} \sim A_k$ (where $\sim$ denotes matrix similarity). Then prove that $A_n \sim A$ for all $n$.  
\end{problem}

Problem \ref{problem:similarity proof} shows that $A_n$ has the same spectrum as $A$. Preservation of eigenvalues is the first important feature that makes the algorithm work. The other important feature is that each iteration of the algorithm effectively transfers some of the``mass" from the lower to the upper triangle. Under very general conditions, $A_n$ will converge to a matrix of the form

\begin{equation}
\label{eq:Schur form}
     \begin{pmatrix}
          B_1 &* & \cdots & * \\
           0     &B_2  &  \ddots & \vdots \\
           \vdots  & \ddots & \ddots & *  \\
           0 & \cdots & 0 & B_m
    \end{pmatrix}
\end{equation}
where $B_i$ is either a $1 \times 1$ or a $2 \times 2$ matrix. The matrix described in \eqref{eq:Schur form} is called the \emph{Schur form} of $A$. If $A$ is complex, or if $A$ is real and symmetric, then its Schur form will be upper triangular; that is, all the $B_i$ will be $1 \times 1$. If $A$ is real but not symmetric, some of its eigenvalues may be complex. These complex eigenvalues occur as conjugate pairs, as the eigenvalues of the $2 \times 2$ $B_i$. If this is the case, \eqref{eq:Schur form} is called the \emph{real Schur form} of $A$ and is not quite upper triangular. It's easy to find the eigenvalues of the Schur form or the real Schur form of $A$. (They're the eigenvalues of the $B_i$.) And by Problem \ref{problem:similarity proof}, these are also the eigenvalues of $A$.
(For more on Schur form, see Lab \ref{Ch:Jordan}.)

The following algorithm is based on the above process but is closer to what is actually done in the real world of numerical computing. The results from Problem \ref{problem:similarity proof} still hold in this new version.

\begin{pseudo}{QR algorithm}{A}
\label{Alg:QR algorithm}
n,n \GETS size(A)\\
I \GETS eye(n,n)\\
 Q_0, A_0 \GETS Hessenberg(A)\\
\COMMENT{gives $Q_0$ unitary and $A_0$ upper Hessenberg s.t. $Q_0 A_0 Q_0^* = A$}\\
\FOR k \GETS 1, 2, ... \DO
\BEGIN
	s_k \GETS A_{k-1}(n,n)\\
	Q_k, R_k \GETS QR(A_{k-1}-s_k I)\\
	A_k = R_k Q_k + s_k I
\END
\end{pseudo}

Notice that we've added a step at the beginning: reduction to upper Hessenberg form. This zeros out all the entries below the subdiagonal of $A$ and speeds up the diagonalization process. We've also added a shift at each step, $s_k$, which further speeds convergence. Setting $s_k = A_{k-1}(n,n)$ is one of many possible shift choices.

\begin{problem}
Code the $QR$ algorithm. Have your function accept an $n \times n$ matrix $A$ and a number of iterations, and return all the eigenvalues of $A$. Use your Hessenberg function from Lab \ref{Ch:Householder Reflections} for the first step. For QR decomposition, use your own implementation from Labs \ref{lab:QRdecomp}, \ref{Ch:Householder Reflections}, or \ref{Ch:Givens Rotations} (Householder is usually fastest), or use the built-in version. Note that you will need to find the eigenvalues of the $2 \times 2$ $B_i$ directly, if there are any.
\end{problem}

\begin{problem}
If $A$ is normal, its Schur form is diagonal. For normal $A$, have your function additionally output the eigenvector corresponding to each eigenvalue. Hint 1: Real symmetric matrices and Hermitian matrices are both normal. Hint 2: Your work in Problem \ref{problem:similarity proof} may be helpful. You have already made all the necessary calculations, you just need to store the information correctly.
\end{problem}

\begin{problem}
Test your implementation with random matrices and random symmetric matrices. Compare your output to the output of \li{linalg.eig}. How many iterations are necessary? How large can $A$ be?
\end{problem}

The $QR$ algorithm is not the only iterative method used to find eigenvalues. For large sparse matrices MATLAB uses the Arnoldi iteration, which is similary to the $QR$ algorithm but exploits sparsity. Other methods include the Jacobi method and the Rayleigh quotient method.

It is important to remember that eigenvalue solvers can be wrong, particularly for matrices that are ill-conditioned. 
\begin{matlab}
For example using the MATLAB built in solver for the matrix generated by {\tt gallery(5)} yields the following output:
\begin{lstlisting}[style=matlab]
>> eig(gallery(5))
ans =
  -0.0405          
  -0.0118 + 0.0383i
  -0.0118 - 0.0383i
   0.0320 + 0.0228i
   0.0320 - 0.0228i
\end{lstlisting}

However, all of the eigenvalues of {\tt gallery(5)} are actually zero. This highlights the need to realize that eigenvalue solvers are really just doing an approximation.
\end{matlab}
